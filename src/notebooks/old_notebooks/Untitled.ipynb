{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2fa20d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import pyro.distributions.transforms as T\n",
    "import pyro.distributions as dist\n",
    "from torch import nn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cc53da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1\n",
    "\n",
    "x = torch.cat([torch.randn(64, d) - 3, torch.randn(64, d) + 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f2552a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/druhe/Projects/gw/.venv/lib/python3.9/site-packages/pyro/nn/auto_reg_nn.py:179: UserWarning: ConditionalAutoRegressiveNN input_dim = 1. Consider using an affine transformation instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "aa = T.affine_autoregressive(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4324fc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dist = dist.Normal(torch.zeros(d), torch.ones(d))\n",
    "num_layers = 8\n",
    "transform = [T.affine_autoregressive(d) for l in range(num_layers)]\n",
    "\n",
    "\n",
    "flow_dist = dist.TransformedDistribution(base_dist, transform)\n",
    "\n",
    "\n",
    "transform_modules = nn.ModuleList([m for m in transform if isinstance(m, nn.Module)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3566258a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "count_parameters(transform_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "668554f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 3.166220188140869\n",
      "step: 500, loss: 2.6234347820281982\n",
      "step: 1000, loss: 2.623434543609619\n",
      "step: 1500, loss: 2.623434543609619\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      4\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 5\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mflow_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#     loss = -log_prob(flow_dist, dataset).mean()\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Projects/gw/.venv/lib/python3.9/site-packages/torch/distributions/transformed_distribution.py:149\u001b[0m, in \u001b[0;36mTransformedDistribution.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    145\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m log_prob \u001b[38;5;241m-\u001b[39m _sum_rightmost(transform\u001b[38;5;241m.\u001b[39mlog_abs_det_jacobian(x, y),\n\u001b[1;32m    146\u001b[0m                                          event_dim \u001b[38;5;241m-\u001b[39m transform\u001b[38;5;241m.\u001b[39mdomain\u001b[38;5;241m.\u001b[39mevent_dim)\n\u001b[1;32m    147\u001b[0m     y \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m--> 149\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m log_prob \u001b[38;5;241m+\u001b[39m _sum_rightmost(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    150\u001b[0m                                      event_dim \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_dist\u001b[38;5;241m.\u001b[39mevent_shape))\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m log_prob\n",
      "File \u001b[0;32m~/Projects/gw/.venv/lib/python3.9/site-packages/torch/distributions/independent.py:91\u001b[0m, in \u001b[0;36mIndependent.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[0;32m---> 91\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _sum_rightmost(log_prob, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreinterpreted_batch_ndims)\n",
      "File \u001b[0;32m~/Projects/gw/.venv/lib/python3.9/site-packages/torch/distributions/normal.py:77\u001b[0m, in \u001b[0;36mNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     75\u001b[0m var \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     76\u001b[0m log_scale \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale, Real) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\u001b[38;5;241m.\u001b[39mlog()\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m(\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m var) \u001b[38;5;241m-\u001b[39m log_scale \u001b[38;5;241m-\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mpi))\n",
      "File \u001b[0;32m~/Projects/gw/.venv/lib/python3.9/site-packages/torch/_tensor.py:31\u001b[0m, in \u001b[0;36m_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steps = 2048\n",
    "optimizer = torch.optim.Adam(transform_modules.parameters(), lr=1e-2)\n",
    "for step in range(steps+1):\n",
    "    optimizer.zero_grad()\n",
    "    loss = -flow_dist.log_prob(x).mean()\n",
    "#     loss = -log_prob(flow_dist, dataset).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    flow_dist.clear_cache()\n",
    "    \n",
    "    if step % 500 == 0:\n",
    "        print('step: {}, loss: {}'.format(step, loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12a1654",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sample = flow_dist.sample((1000,)).squeeze().numpy()\n",
    "    sns.kdeplot(sample)\n",
    "    plt.title(f\"Mean: {sample.mean()}, STD: {sample.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e145b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fda64e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def gradient(y, x, grad_outputs=None):\n",
    "    \"\"\"Compute dy/dx @ grad_outputs\"\"\"\n",
    "    if grad_outputs is None:\n",
    "        grad_outputs = torch.ones_like(y)\n",
    "    grad = torch.autograd.grad(y, [x], grad_outputs = grad_outputs, create_graph=True)[0]\n",
    "    return grad\n",
    "\n",
    "def jacobian(y, x):\n",
    "    \"\"\"Compute dy/dx = dy/dx @ grad_outputs; \n",
    "    for grad_outputs in [1, 0, ..., 0], [0, 1, 0, ..., 0], ...., [0, ..., 0, 1]\"\"\"\n",
    "    jac = torch.zeros(y.shape[0], x.shape[0]) \n",
    "    for i in range(y.shape[0]):\n",
    "        grad_outputs = torch.zeros_like(y)\n",
    "        grad_outputs[i] = 1\n",
    "        jac[i] = gradient(y, x, grad_outputs = grad_outputs)\n",
    "    return jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2de6821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "215b1d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Sequential(nn.Linear(2, 2), nn.LeakyReLU(), nn.Linear(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f7eae6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aef894ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.functional import jacobian\n",
    "def exp_reducer(x):\n",
    "    return linear(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3d3d6efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0003, -0.0007],\n",
       "          [ 0.0000,  0.0000]],\n",
       "\n",
       "         [[-0.0009,  0.0008],\n",
       "          [ 0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000,  0.0000],\n",
       "          [-0.0107, -0.0085]],\n",
       "\n",
       "         [[ 0.0000,  0.0000],\n",
       "          [-0.0280, -0.0187]]]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian(exp_reducer, torch.randn(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "40ca300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fdd4103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_jacobian(func, x, create_graph=False):\n",
    "  # x in shape (Batch, Length)\n",
    "  def _func_sum(x):\n",
    "    return func(x).sum(dim=0)\n",
    "  return autograd.functional.jacobian(_func_sum, x, create_graph=create_graph, vectorize=True).permute(1,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "579d6a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "J = batch_jacobian(exp_reducer, torch.rand(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fa4eccfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 2])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1fe61beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.9905e-05, -3.9905e-05])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.det(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840b2d92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
