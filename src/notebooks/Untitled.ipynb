{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54639fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d3/x840qlg17x1f92cnsrkq62fw0000gn/T/ipykernel_3236/1774771994.py:53: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matricesor `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at  /Users/distiller/project/pytorch/aten/src/ATen/native/TensorShape.cpp:2318.)\n",
      "  input_data = torch.from_numpy(np.array([data['m1'],data['m2']/data['m1']])).float().T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi11\n",
      "0\n",
      "tensor([359.3786], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "1\n",
      "tensor([359.3684], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "2\n",
      "tensor([359.3583], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "3\n",
      "tensor([359.3483], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "4\n",
      "tensor([359.3382], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "5\n",
      "tensor([359.3280], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "6\n",
      "tensor([359.3175], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "7\n",
      "tensor([359.3068], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "8\n",
      "tensor([359.2957], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "9\n",
      "tensor([359.2842], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "10\n",
      "tensor([359.2721], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "11\n",
      "tensor([359.2595], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "12\n",
      "tensor([359.2461], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "13\n",
      "tensor([359.2320], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "14\n",
      "tensor([359.2169], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "15\n",
      "tensor([359.2009], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "16\n",
      "tensor([359.1838], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "17\n",
      "tensor([359.1655], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "18\n",
      "tensor([359.1459], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "19\n",
      "tensor([359.1248], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "20\n",
      "tensor([359.1022], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "21\n",
      "tensor([359.0779], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "22\n",
      "tensor([359.0517], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "23\n",
      "tensor([359.0234], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "24\n",
      "tensor([358.9930], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "25\n",
      "tensor([358.9602], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "26\n",
      "tensor([358.9247], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "27\n",
      "tensor([358.8864], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "28\n",
      "tensor([358.8450], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "29\n",
      "tensor([358.8003], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "30\n",
      "tensor([358.7520], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "31\n",
      "tensor([358.6998], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "32\n",
      "tensor([358.6433], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "33\n",
      "tensor([358.5823], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "34\n",
      "tensor([358.5163], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "35\n",
      "tensor([358.4449], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "36\n",
      "tensor([358.3677], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "37\n",
      "tensor([358.2841], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "38\n",
      "tensor([358.1938], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "39\n",
      "tensor([358.0962], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "40\n",
      "tensor([357.9906], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "41\n",
      "tensor([357.8765], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "42\n",
      "tensor([357.7531], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "43\n",
      "tensor([357.6197], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "44\n",
      "tensor([357.4757], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "45\n",
      "tensor([357.3201], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "46\n",
      "tensor([357.1521], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "47\n",
      "tensor([356.9708], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "48\n",
      "tensor([356.7751], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "49\n",
      "tensor([356.5641], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "50\n",
      "tensor([356.3367], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "51\n",
      "tensor([356.0917], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "52\n",
      "tensor([355.8280], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "53\n",
      "tensor([355.5441], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "54\n",
      "tensor([355.2390], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "55\n",
      "tensor([354.9113], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "56\n",
      "tensor([354.5594], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "57\n",
      "tensor([354.1823], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "58\n",
      "tensor([353.7782], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "59\n",
      "tensor([353.3459], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "60\n",
      "tensor([352.8839], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "61\n",
      "tensor([352.3907], grad_fn=<NegBackward0>)\n",
      "hi\n",
      "hi11\n",
      "62\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 85>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mgeometric_term\u001b[38;5;66;03m#(exponential_term+geometric_term)\u001b[39;00m\n\u001b[1;32m     94\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 95\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "File \u001b[0;32m~/Projects/gw/.venv/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/gw/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.functional import meshgrid, norm\n",
    "from torch.nn.modules import normalization\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Swish(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, hidden_layers=5):\n",
    "        super().__init__()\n",
    "#        self.normalization = nn.Parameter(torch.randn(1))\n",
    "        layers = np.array([nn.Linear(hidden_size, hidden_size) for i in range(hidden_layers)])\n",
    "        layers = np.insert(layers,np.arange(1,hidden_layers+1),Swish())\n",
    "        self.output = nn.Sequential(\n",
    "        nn.Linear(input_size, hidden_size), nn.LayerNorm(hidden_size), Swish(),\n",
    "        *layers,\n",
    "        nn.Linear(hidden_size, output_size),nn.Softplus())\n",
    "\n",
    "    def compute_normalization(self, axis):\n",
    "        n_dim = len(axis)\n",
    "        grid = torch.from_numpy(np.array(np.meshgrid(*axis)).reshape(n_dim,-1).T).float()\n",
    "        value = self.unnormalized_pass(grid)\n",
    "        shape = []\n",
    "        for i in range(n_dim):\n",
    "            shape.append(len(axis[i]))\n",
    "        norm = value.T.reshape(shape)\n",
    "        for i in range(n_dim):\n",
    "            norm = torch.trapz(norm,x=torch.from_numpy(axis[i]).float(),axis=0)\n",
    "        return norm\n",
    "\n",
    "    def unnormalized_pass(self, x):\n",
    "        return self.output(x)\n",
    "\n",
    "    def forward(self, x, axis):\n",
    "        return self.output(x)/self.compute_normalization(axis)\n",
    "\n",
    "\n",
    "n_dim = 2\n",
    "model = MLP(n_dim, 1, 128, 5)\n",
    "model.train().float()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "\n",
    "data = np.load('../datasets/Combined_GWTC_m1m2chieffz.npz')\n",
    "\n",
    "input_data = torch.from_numpy(np.array([data['m1'],data['m2']/data['m1']])).float().T\n",
    "#input_data = torch.from_numpy(np.array([data['m1']])).float().T\n",
    "\n",
    "\n",
    "n_epoch = 10000\n",
    "\n",
    "n_grid = 100\n",
    "print('hi')\n",
    "m1_axis = 10**np.linspace(np.log10(0.5),np.log10(100),n_grid)\n",
    "q_axis = np.linspace(0.05,1,n_grid)\n",
    "axis = [m1_axis,q_axis]\n",
    "grid_flatten = torch.from_numpy(np.array(np.meshgrid(*axis)).reshape(n_dim,-1).T).float()\n",
    "print('hi')\n",
    "value = model(grid_flatten,axis)\n",
    "shape = []\n",
    "for i in range(n_dim):\n",
    "    shape.append(len(axis[i]))\n",
    "print('hi')\n",
    "norm = value.reshape(shape).T\n",
    "grid = grid_flatten.reshape(*shape,n_dim).T\n",
    "print('hi')\n",
    "# def compute_normalization(axis,model):\n",
    "#     grid = torch.from_numpy(np.array(np.meshgrid(*axis)).reshape(n_dim,-1).T).float().\n",
    "#     value = model(grid)\n",
    "#     shape = []\n",
    "#     for i in range(n_dim):\n",
    "#         shape.append(len(axis[i]))\n",
    "#     norm = value.T.reshape(shape)\n",
    "#     for i in range(n_dim):\n",
    "#         norm = torch.trapz(norm,x=torch.from_numpy(axis[i]).float(),axis=0)\n",
    "#     return norm\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    # Write training loop for MLP\n",
    "    print('hi')\n",
    "    output = model(input_data,axis)\n",
    "    print('hi11')\n",
    "    geometric_term = torch.sum(torch.log(torch.mean(output,dim=0)),dim=0)\n",
    "#    exponential_term = data['m1'].shape[0]*torch.log(model.normalization)-model.normalization\n",
    "    print(i)\n",
    "    loss = -geometric_term#(exponential_term+geometric_term)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)\n",
    "\n",
    "def poisson_likelihood(N,mu):\n",
    "    return N*np.log(mu)-mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e86ffaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = torch.from_numpy(np.array(np.meshgrid(*axis)).reshape(n_dim,-1).T).float()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96518411",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
